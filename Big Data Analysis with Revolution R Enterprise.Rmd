---
title: "Big Data Analysis with Revolution R Enterprise"
author: "DataCamp Big Data Analysis Course"
date: "29. Juni 2015"
output: html_document
---

[Big Data Analysis with Revolution R Enterprise](https://www.datacamp.com/courses/big-data-revolution-r-enterprise-tutorial)

### RevoScaleR Course 
Big Data Analysis with Revolution R Enterprise
Revolution R Enterprise allows R users to process, visualize, and model 
terabyte-class data sets at a fraction of the time of legacy products 
without requiring expensive or specialized hardware. Introductory course 
for accomplished R users to experience the functionality of Revolution R Enterprise.

***

# CHAPTER 1
##Introduction
##Big Data Analytics Overview

Question: **Do delayed flights fly faster?** Using big data from the 2007 airflight as data-source. Run data exploration, histograms

```{r eval=FALSE}
> rxGetInfo(data = myAirlineXdf, getVarInfo = TRUE, numRows = 10)
```

```
File name: /tmp/Rserv/conn18419/2007_subset.xdf 
Number of observations: 149139 
Number of variables: 5 
Number of blocks: 1 
Compression type: zlib 
Variable information: 
Var 1: ActualElapsedTime, Type: integer, Low/High: (17, 730)
Var 2: AirTime, Type: integer, Low/High: (0, 704)
Var 3: Distance, Type: integer, Low/High: (31, 4962)
Var 4: DepDelay, Type: integer, Low/High: (-38, 1243)
Var 5: ArrDelay, Type: integer, Low/High: (-63, 1262)
Data (10 rows starting with row 1):
   ActualElapsedTime AirTime Distance DepDelay ArrDelay
1                 62      44      237       24       26
2                191     179     1262       -3      -27
3                138     127     1111       24        2
4                131     110      842       18       19
5                138     123      889       92       75
6                 82      58      410       -3       -1
7                131     118      822       -1      -20
8                 61      48      335        0       -4
9                 66      55      325       -5       -9
10               295     279     2106       12      -23
```

```{r eval=FALSE}
> # Summarize the variables corresponding to actual elapsed time, time in the air, departure delay, flight Distance.
> rxSummary(formula = ~ActualElapsedTime + AirTime + DepDelay + Distance, data = myAirlineXdf)
```

```
Call:
rxSummary(formula = ~ActualElapsedTime + AirTime + DepDelay + 
    Distance, data = myAirlineXdf)

Summary Statistics Results for: ~ActualElapsedTime + AirTime + DepDelay
    + Distance
Data: myAirlineXdf (RxXdfData Data Source)
File name: 2007_subset.xdf
Number of valid observations: 149139 
 
 Name              Mean      StdDev    Min Max  ValidObs MissingObs
 ActualElapsedTime 126.07290  71.07911  17  730 145568   3571      
 AirTime           102.54263  68.07955   0  704 145568   3571      
 DepDelay           11.36742  36.05140 -38 1243 145892   3247      
 Distance          717.78808 560.99114  31 4962 149139      0   
```

```{r eval=FALSE}
> # Histogram of departure delays
> rxHistogram(formula = ~DepDelay, data = myAirlineXdf)
> # Use parameters similar to a regular histogram to zero in on the interesting area
> rxHistogram(formula = ~DepDelay, data = myAirlineXdf, 
              xAxisMinMax = c(-100, 400), numBreaks = 500, xNumTicks = 10)
```


## Adding another variable

```{r eval=FALSE}
> ## Calculate an additional variable: airspeed (distance traveled / time in the air). 
> rxDataStep(inData = myAirlineXdf, outFile = myAirlineXdf, 
             varsToKeep = c("Distance", "AirTime"), 
             transforms = list(airSpeed = Distance/AirTime), 
             append = "rows", overwrite = T)
> # Get Variable Information for airspeed
> rxGetInfo(data = myAirlineXdf, getVarInfo = TRUE, 
            varsToKeep = "airSpeed")
```

```
File name: /tmp/Rserv/conn18419/2007_subset.xdf 
Number of observations: 149139 
Number of variables: 6 
Number of blocks: 1 
Compression type: zlib 
Variable information: 
Var 1: airSpeed, Type: numeric, Low/High: (0.7436, 833.0000)
```

```{r eval=FALSE}
# Summary for the airspeed variable
> rxSummary(formula = ~airSpeed, data = myAirlineXdf)
```

```
Call:
rxSummary(formula = ~airSpeed, data = myAirlineXdf)

Summary Statistics Results for: ~airSpeed
Data: myAirlineXdf (RxXdfData Data Source)
File name: 2007_subset.xdf
Number of valid observations: 149139 
 
 Name     Mean     StdDev   Min       Max ValidObs MissingObs
 airSpeed 6.564265 3.008519 0.7435897 833 145565   3574 
```

```{r eval=FALSE}
> # Construct a histogram for airspeed
> # We can use the xAxisMinMax argument to limit the X-axis.
> rxHistogram(formula = ~airSpeed, data = myAirlineXdf)
> rxHistogram(formula = ~airSpeed, data = myAirlineXdf, 
            xNumTicks = 10, 
            numBreaks = 1500, 
            xAxisMinMax = c(0, 12))
```

## Tweaking airSpeed

```{r eval=FALSE}
> 
> # Conversion to miles per hour (from miles per minute)
> rxDataStep(inData = myAirlineXdf, outFile = myAirlineXdf, 
            varsToKeep = c("airSpeed"), 
            transforms = list(airSpeed = airSpeed * 60), 
            overwrite = TRUE)
> # Histogram for airspeed after conversion
> rxHistogram(formula = ~airSpeed, data = myAirlineXdf)
```

## Is there a strong correlation between airSpeed and DepDelay/ArrDelay?

```{r eval=FALSE}
> 
> # Correlation for departure delay, arrival delay, and air speed
> rxCor(formula = ~DepDelay + ArrDelay + airSpeed, 
            data = myAirlineXdf, 
            rowSelection = (airSpeed > 50) & (airSpeed < 800))
```

```
           DepDelay    ArrDelay    airSpeed
DepDelay 1.00000000  0.93157838  0.02196339
ArrDelay 0.93157838  1.00000000 -0.06668119
airSpeed 0.02196339 -0.06668119  1.00000000
```

## Is there a linear relationship between airSpeed and DepDelay?

```{r eval=FALSE}
> 
> # Regression for airSpeed based on departure delay
> myLMobj <- rxLinMod(formula = airSpeed ~ DepDelay, 
            data = myAirlineXdf, 
            rowSelection = (airSpeed > 50) & (airSpeed < 800))
> summary(myLMobj)
```

```
Call:
rxLinMod(formula = airSpeed ~ DepDelay, data = myAirlineXdf, 
    rowSelection = (airSpeed > 50) & (airSpeed < 800))

Linear Regression Results for: airSpeed ~ DepDelay
Data: myAirlineXdf (RxXdfData Data Source)
File name: 2007_subset.xdf
Dependent variable(s): airSpeed
Total independent variables: 2 
Number of valid observations: 145054
Number of missing observations: 0 
 
Coefficients:
             Estimate Std. Error  t value Pr(>|t|)    
(Intercept) 3.901e+02  2.091e-01 1866.095 2.22e-16 ***
DepDelay    4.638e-02  5.543e-03    8.367 2.22e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 75.95 on 145052 degrees of freedom
Multiple R-squared: 0.0004824 
Adjusted R-squared: 0.0004755 
F-statistic: 70.01 on 1 and 145052 DF,  p-value: < 2.2e-16 
Condition number: 1 
```

A low p factor, factor appears significant but the DepDelay is affected by a very low coefficient
the next question is **if the relationship is non-linear**, for example if flights with longer delays
do not "rush" anymore, and flights with shorter delays do "rush" to catch up.
this requires splitting the data on 10-fold intervals 

***

# CHAPTER 2
## Data Exploration 
## exploring the sample Dow Jones data


```{r eval=FALSE}
> 
> ## extract the names of the possible options:
> names(rxOptions())
```

```
 [1] "cintSysDir"            "includeDir"            "libDir"               
 [4] "linkDllName"           "unitTestDir"           "unitTestDataDir"      
 [7] "sampleDataDir"         "demoScriptsDir"        "fileSystem"           
[10] "hdfsPort"              "hdfsHost"              "xdfCompressionLevel"  
[13] "computeContext"        "blocksPerRead"         "reportProgress"       
[16] "rowDisplayMax"         "memStatsReset"         "memStatsDiff"         
[19] "numCoresToUse"         "numDigits"             "showTransformFn"      
[22] "defaultDecimalColType" "defaultMissingColType" "dataPath"             
[25] "outDataPath"           "transformPackages"     "useSparseCube"        
[28] "dropMain"              "coefLabelStyle"   
```

```{r eval=FALSE}
> 
> ## extract the sample data directory:
> rxGetOption("sampleDataDir")
```

```
[1] "/usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData"
```


```{r eval=FALSE}
> ## view the current value of the reportProgress option
> rxGetOption("reportProgress")
[1] 2
> ## set the value of the reportProgress option to 0
> rxOptions(reportProgress = 0)
> 
> ## set up the variable that has the address of the relevant data file:
> djiXdf <- file.path(rxGetOption("sampleDataDir"), "DJIAdaily.xdf")
> ## get information about that dataset:
> rxGetInfo(djiXdf, getVarInfo = TRUE)
```

```
File name: /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf 
Number of observations: 20636 
Number of variables: 13 
Number of blocks: 4 
Compression type: zlib 
Variable information: 
Var 1: Date, Type: character
Var 2: Open, Type: numeric, Storage: float32, Low/High: (41.6300, 14165.0195)
Var 3: High, Type: numeric, Storage: float32, Low/High: (42.6100, 14279.9600)
Var 4: Low, Type: numeric, Storage: float32, Low/High: (40.5600, 13980.9004)
Var 5: Close, Type: numeric, Storage: float32, Low/High: (41.2200, 14164.5303)
Var 6: Volume, Type: numeric, Storage: float32, Low/High: (130000.0000, 11456230400.0000)
Var 7: Adj.Close, Type: numeric, Storage: float32, Low/High: (41.2200, 14164.5303)
Var 8: Year, Type: integer, Low/High: (1928, 2010)
Var 9: Month, Type: integer, Low/High: (1, 12)
Var 10: DayOfMonth, Type: integer, Low/High: (1, 31)
Var 11: DayOfWeek
       5 factor levels: Monday Tuesday Wednesday Thursday Friday
Var 12: DaysSince1928, Type: integer, Low/High: (274, 30286)
Var 13: YearFrac, Type: numeric, Storage: float32, Low/High: (1928.7501, 2010.9186)
```

```{r eval=FALSE}
> rxGetVarInfo(djiXdf)
```

```
Var 1: Date, Type: character
Var 2: Open, Type: numeric, Storage: float32, Low/High: (41.6300, 14165.0195)
Var 3: High, Type: numeric, Storage: float32, Low/High: (42.6100, 14279.9600)
Var 4: Low, Type: numeric, Storage: float32, Low/High: (40.5600, 13980.9004)
Var 5: Close, Type: numeric, Storage: float32, Low/High: (41.2200, 14164.5303)
Var 6: Volume, Type: numeric, Storage: float32, Low/High: (130000.0000, 11456230400.0000)
Var 7: Adj.Close, Type: numeric, Storage: float32, Low/High: (41.2200, 14164.5303)
Var 8: Year, Type: integer, Low/High: (1928, 2010)
Var 9: Month, Type: integer, Low/High: (1, 12)
Var 10: DayOfMonth, Type: integer, Low/High: (1, 31)
Var 11: DayOfWeek
       5 factor levels: Monday Tuesday Wednesday Thursday Friday
Var 12: DaysSince1928, Type: integer, Low/High: (274, 30286)
Var 13: YearFrac, Type: numeric, Storage: float32, Low/High: (1928.7501, 2010.9186)
```

```{r eval=FALSE}
> ## get variable information for the dataset
> djiVarInfo <- rxGetVarInfo(djiXdf)
> names(djiVarInfo)
```

```
 [1] "Date"          "Open"          "High"          "Low"          
 [5] "Close"         "Volume"        "Adj.Close"     "Year"         
 [9] "Month"         "DayOfMonth"    "DayOfWeek"     "DaysSince1928"
[13] "YearFrac"  
```

```{r eval=FALSE}
> ## extract information about the closing cost variable
> (closeVarInfo <- djiVarInfo$Close)
Type: numeric, Storage: float32, Low/High: (41.2200, 14164.5303)
> ## get the class of the closeVarInfo object:
> class(closeVarInfo)
[1] "rxVarInfo"
> ## examine the structure of the closeVarInfo object:
> str(closeVarInfo)
```

```
List of 4
 $ varType: chr "numeric"
 $ storage: chr "float32"
 $ low    : num 41.2
 $ high   : num 14165
 - attr(*, "class")= chr "rxVarInfo"
```

```{r eval=FALSE}
> ## extract the global maximum of the closing cost variable:
> closeMax <- closeVarInfo[["high"]]
```


```{r eval=FALSE}
> ## Basic summary statistics:
> rxSummary(~DayOfWeek + Close + Volume, data = djiXdf)
```

```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: Less than .001 seconds 
Computation time: 0.007 seconds.
Call:
rxSummary(formula = ~DayOfWeek + Close + Volume, data = djiXdf)

Summary Statistics Results for: ~DayOfWeek + Close + Volume
Data: djiXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Number of valid observations: 20636 
 
 Name   Mean         StdDev      Min       Max          ValidObs MissingObs
 Close  2.516781e+03 3.65031e+03     41.22 1.416453e+04 20636    0         
 Volume 4.173323e+08 1.09826e+09 130000.00 1.145623e+10 20636    0         

Category Counts for DayOfWeek
Number of categories: 5
Number of valid observations: 20636
Number of missing observations: 0

 DayOfWeek Counts
 Monday    3989  
 Tuesday   4182  
 Wednesday 4205  
 Thursday  4139  
 Friday    4121  
```


```{r eval=FALSE}
> ## Frequency weighted:
> rxSummary(~DayOfWeek + Close + Volume, data = djiXdf, fweights = "Volume")
```

```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.002 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.001 seconds 
Computation time: 0.008 seconds.
Call:
rxSummary(formula = ~DayOfWeek + Close + Volume, data = djiXdf, 
    fweights = "Volume")

Summary Statistics Results for: ~DayOfWeek + Close + Volume
Data: djiXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Frequency weights: Volume
Sum of weights of valid observations: 20636 
 
 Name      Mean         StdDev       Min       Max          SumOfWeights
 DayOfWeek 2.046997e+00 1.380471e+00      0.00 4.000000e+00 8.61207e+12 
 Close     9.681542e+03 2.712059e+03     41.22 1.416453e+04 8.61207e+12 
 Volume    3.307396e+09 2.289756e+09 130000.00 1.145623e+10 8.61207e+12 
 MissingWeights
 0             
 0             
 0             

Category Counts for DayOfWeek
Number of categories: 5

 DayOfWeek Counts      
 Monday    1.512617e+12
 Tuesday   1.774277e+12
 Wednesday 1.821944e+12
 Thursday  1.802208e+12
 Friday    1.701024e+12
```

```{r eval=FALSE}
> ## Basic frequency count:
> rxCrossTabs(~DayOfWeek, data = djiXdf)
```


```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: Less than .001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: Less than .001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: Less than .001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: Less than .001 seconds 
Computation time: 0.005 seconds.
Call:
rxCrossTabs(formula = ~DayOfWeek, data = djiXdf)

Cross Tabulation Results for: ~DayOfWeek
Data: djiXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Number of valid observations: 20636
Number of missing observations: 0 
Statistic: counts 
 
DayOfWeek (counts):
              
Monday    3989
Tuesday   4182
Wednesday 4205
Thursday  4139
Friday    4121
```


```{r eval=FALSE}
> ## Numeric Variables
> rxHistogram(~Close, data = djiXdf)
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.004 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.004 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.004 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.003 seconds 
Computation time: 0.018 seconds.
> ## Categorical Variable:
> rxHistogram(~DayOfWeek, data = djiXdf)
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: Less than .001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: Less than .001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: Less than .001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: Less than .001 seconds 
Computation time: 0.004 seconds.
> ## Different panels for different days of the week
> rxHistogram(~Close | DayOfWeek, data = djiXdf)
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.005 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.005 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.004 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.003 seconds 
Computation time: 0.021 seconds.
> ## Numeric Variables with a frequency weighting:
> rxHistogram(~Close, data = djiXdf, fweights = "Volume")
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.004 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.005 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.004 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.003 seconds 
Computation time: 0.021 seconds.
> 
```


```{r eval=FALSE}
> ## Simple bivariate line plot:
> rxLinePlot(Close ~ DaysSince1928, data = djiXdf)
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.003 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.002 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.002 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.001 seconds 
> ## Using different panels for different days of the week:
> rxLinePlot(Close ~ DaysSince1928 | DayOfWeek, data = djiXdf)
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.003 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.002 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.002 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.002 seconds 
> ## Using different groups.
> rxLinePlot(Close ~ DaysSince1928, groups = DayOfWeek, data = djiXdf)
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.003 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.003 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.003 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.002 seconds 
> ## Simple bivariate line plot, after taking the log() of the ordinate (y) variable.
> rxLinePlot(log(Close) ~ DaysSince1928, data = djiXdf)
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.003 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.002 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.002 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.001 seconds 
> 
```

```{r eval=FALSE}
> ## Compute the the summed volume for each day of the week for each month:
> rxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, data = djiXdf)
```


```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: Less than .001 seconds 
Computation time: 0.007 seconds.
Call:
rxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, data = djiXdf)

Cross Tabulation Results for: Volume ~ F(Month):DayOfWeek
Data: djiXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Dependent variable(s): Volume
Number of valid observations: 20636
Number of missing observations: 0 
Statistic: sums 
 
Volume (sums):
       DayOfWeek
F_Month       Monday      Tuesday    Wednesday     Thursday       Friday
     1  100453160640 140286240336 149087369328 152184470496 147892141520
     2   91926419616 134865669392 133602339728 135367399680 139232921600
     3  155211019552 158054359936 160485980304 156809870400 143296529568
     4  135360420160 149625269760 158095860384 160039070288 126733750400
     5  114411520096 152175269984 158420289616 160328410080 149583789488
     6  135188510400 147547849776 145226389792 143743650048 149822300080
     7  125614759936 152701159648 157332601008 163020899488 134820430224
     8  132849590176 138982279296 148299859456 147770430432 138013450560
     9  112979300912 157709201312 157600099424 151305940528 144968749200
     10 145755510672 162156030160 173506599040 176610670944 174333560160
     11 147301549648 155582169504 150121160240 130254850080 132521269088
     12 115565549936 124591890400 130164970256 124772270800 119805110368
```

```{r eval=FALSE}
> ## Compute the the average volume for each day of the week for each month:
> rxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, data = djiXdf, mean = TRUE
```


```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: Less than .001 seconds 
Computation time: 0.007 seconds.
Call:
rxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, data = djiXdf, 
    means = TRUE)

Cross Tabulation Results for: Volume ~ F(Month):DayOfWeek
Data: djiXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Dependent variable(s): Volume
Number of valid observations: 20636
Number of missing observations: 0 
Statistic: means 
 
Volume (means):
       DayOfWeek
F_Month    Monday   Tuesday Wednesday  Thursday    Friday
     1  308138530 400817830 423543663 433573990 420148129
     2  340468221 418837483 417507312 421705295 435102880
     3  429947423 437823712 443331437 434376372 423954229
     4  385642223 426282820 450415557 454656450 432538397
     5  367882701 427458624 442514775 450360702 419002211
     6  384058268 419170028 417317212 409526069 428063715
     7  370545015 435046039 450809745 464447007 394211784
     8  365976832 382871293 415405769 408205609 380202343
     9  418441855 449313964 452873849 432302687 415383236
     10 406004208 444263096 481962775 485194151 478938352
     11 423280315 505136914 427695613 482425371 376480878
     12 340901327 353954234 372965531 356492202 351334635
```

```{r eval=FALSE}
> ## Compute the the average closing price for each day of the week for each month, using volume as frequency weights
> rxCrossTabs(formula = Close ~ F(Month):DayOfWeek, data = djiXdf, means = TRUE, fweights = "Volume")
```

```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.002 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.002 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.002 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.001 seconds 
Computation time: 0.009 seconds.
Call:
rxCrossTabs(formula = Close ~ F(Month):DayOfWeek, data = djiXdf, 
    fweights = "Volume", means = TRUE)

Cross Tabulation Results for: Close ~ F(Month):DayOfWeek
Data: djiXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Frequency weights: Volume
Dependent variable(s): Close
Sum of weights of valid observations: 8612070155344
Number of missing observations: 0 
Statistic: means 
 
Close (means):
       DayOfWeek
F_Month   Monday  Tuesday Wednesday Thursday   Friday
     1  9447.143 9609.819  9713.556 9666.235 9520.421
     2  9318.145 9385.130  9328.339 9265.331 9356.839
     3  9290.767 9281.642  9352.612 9407.788 9292.174
     4  9722.881 9678.512  9668.298 9625.000 9820.420
     5  9906.000 9887.023  9847.209 9908.007 9721.381
     6  9752.588 9721.678  9770.470 9815.480 9878.071
     7  9798.827 9854.674  9645.457 9798.228 9759.297
     8  9843.677 9790.362  9987.167 9968.487 9942.661
     9  9889.898 9893.286  9826.807 9836.212 9875.476
     10 9729.465 9653.250  9666.057 9518.674 9523.039
     11 9819.758 9832.750  9757.541 9858.042 9770.669
     12 9473.085 9486.524  9671.284 9701.425 9642.693
```

```{r eval=FALSE}
> ## Compute the the summed volume for each day of the week:
> rxCrossTabs(Volume ~ DayOfWeek, data = djiXdf)
```

```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: Less than .001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: Less than .001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: Less than .001 seconds 
Computation time: 0.006 seconds.
Call:
rxCrossTabs(formula = Volume ~ DayOfWeek, data = djiXdf)

Cross Tabulation Results for: Volume ~ DayOfWeek
Data: djiXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Dependent variable(s): Volume
Number of valid observations: 20636
Number of missing observations: 0 
Statistic: sums 
 
Volume (sums):
                      
Monday    1.512617e+12
Tuesday   1.774277e+12
Wednesday 1.821944e+12
Thursday  1.802208e+12
Friday    1.701024e+12
```

```{r eval=FALSE}
# or use the similar function rxCube
> rxCube(Volume ~ DayOfWeek, data = djiXdf, means = FALSE)
```

```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: Less than .001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: Less than .001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: Less than .001 seconds 
Computation time: 0.006 seconds.
Call:
rxCube(formula = Volume ~ DayOfWeek, data = djiXdf, means = FALSE)

Cube Results for: Volume ~ DayOfWeek
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Dependent variable(s): Volume
Number of valid observations: 20636
Number of missing observations: 0 
Statistic: Volume sums 
 
  DayOfWeek Volume       Counts
1 Monday    1.512617e+12 3989  
2 Tuesday   1.774277e+12 4182  
3 Wednesday 1.821944e+12 4205  
4 Thursday  1.802208e+12 4139  
5 Friday    1.701024e+12 4121
```

```{r eval=FALSE}
> ## Compute the the summed volume for each day of the week for each month:
> rxCrossTabs(Volume ~ F(Month):DayOfWeek, data = djiXdf)
```

```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.001 seconds 
Computation time: 0.007 seconds.
Call:
rxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, data = djiXdf)

Cross Tabulation Results for: Volume ~ F(Month):DayOfWeek
Data: djiXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Dependent variable(s): Volume
Number of valid observations: 20636
Number of missing observations: 0 
Statistic: sums 
 
Volume (sums):
       DayOfWeek
F_Month       Monday      Tuesday    Wednesday     Thursday       Friday
     1  100453160640 140286240336 149087369328 152184470496 147892141520
     2   91926419616 134865669392 133602339728 135367399680 139232921600
     3  155211019552 158054359936 160485980304 156809870400 143296529568
     4  135360420160 149625269760 158095860384 160039070288 126733750400
     5  114411520096 152175269984 158420289616 160328410080 149583789488
     6  135188510400 147547849776 145226389792 143743650048 149822300080
     7  125614759936 152701159648 157332601008 163020899488 134820430224
     8  132849590176 138982279296 148299859456 147770430432 138013450560
     9  112979300912 157709201312 157600099424 151305940528 144968749200
     10 145755510672 162156030160 173506599040 176610670944 174333560160
     11 147301549648 155582169504 150121160240 130254850080 132521269088
     12 115565549936 124591890400 130164970256 124772270800 119805110368
```

```{r eval=FALSE}
# Month as.factor
> rxCube(Volume ~ F(Month):DayOfWeek, data = djiXdf, means = FALSE)
```

```
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.001 seconds 
Computation time: 0.007 seconds.
Call:
rxCube(formula = Volume ~ F(Month):DayOfWeek, data = djiXdf, 
    means = FALSE)

Cube Results for: Volume ~ F(Month):DayOfWeek
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Dependent variable(s): Volume
Number of valid observations: 20636
Number of missing observations: 0 
Statistic: Volume sums 
 
   F_Month DayOfWeek Volume       Counts
1  1       Monday    100453160640 326   
2  2       Monday     91926419616 270   
3  3       Monday    155211019552 361   
4  4       Monday    135360420160 351   
5  5       Monday    114411520096 311   
6  6       Monday    135188510400 352   
7  7       Monday    125614759936 339   
8  8       Monday    132849590176 363   
9  9       Monday    112979300912 270   
10 10      Monday    145755510672 359   
11 11      Monday    147301549648 348   
12 12      Monday    115565549936 339   
13 1       Tuesday   140286240336 350   
14 2       Tuesday   134865669392 322   
15 3       Tuesday   158054359936 361   
16 4       Tuesday   149625269760 351   
17 5       Tuesday   152175269984 356   
18 6       Tuesday   147547849776 352   
19 7       Tuesday   152701159648 351   
20 8       Tuesday   138982279296 363   
21 9       Tuesday   157709201312 351   
22 10      Tuesday   162156030160 365   
23 11      Tuesday   155582169504 308   
24 12      Tuesday   124591890400 352   
25 1       Wednesday 149087369328 352   
26 2       Wednesday 133602339728 320   
27 3       Wednesday 160485980304 362   
28 4       Wednesday 158095860384 351   
29 5       Wednesday 158420289616 358   
30 6       Wednesday 145226389792 348   
31 7       Wednesday 157332601008 349   
32 8       Wednesday 148299859456 357   
33 9       Wednesday 157600099424 348   
34 10      Wednesday 173506599040 360   
35 11      Wednesday 150121160240 351   
36 12      Wednesday 130164970256 349   
37 1       Thursday  152184470496 351   
38 2       Thursday  135367399680 321   
39 3       Thursday  156809870400 361   
40 4       Thursday  160039070288 352   
41 5       Thursday  160328410080 356   
42 6       Thursday  143743650048 351   
43 7       Thursday  163020899488 351   
44 8       Thursday  147770430432 362   
45 9       Thursday  151305940528 350   
46 10      Thursday  176610670944 364   
47 11      Thursday  130254850080 270   
48 12      Thursday  124772270800 350   
49 1       Friday    147892141520 352   
50 2       Friday    139232921600 320   
51 3       Friday    143296529568 338   
52 4       Friday    126733750400 293   
53 5       Friday    149583789488 357   
54 6       Friday    149822300080 350   
55 7       Friday    134820430224 342   
56 8       Friday    138013450560 363   
57 9       Friday    144968749200 349   
58 10      Friday    174333560160 364   
59 11      Friday    132521269088 352   
60 12      Friday    119805110368 341
```


```{r eval=FALSE}
> ## Compute the the average volume for each day of the week for each month:
> rxCube(Volume ~ F(Month):DayOfWeek, data = djiXdf)
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.001 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.001 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: Less than .001 seconds 
Computation time: 0.007 seconds.
Call:
rxCube(formula = Volume ~ F(Month):DayOfWeek, data = djiXdf)

Cube Results for: Volume ~ F(Month):DayOfWeek
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Dependent variable(s): Volume
Number of valid observations: 20636
Number of missing observations: 0 
Statistic: Volume means 
 
   F_Month DayOfWeek Volume    Counts
1  1       Monday    308138530 326   
2  2       Monday    340468221 270   
3  3       Monday    429947423 361   
4  4       Monday    385642223 351   
5  5       Monday    367882701 311   
6  6       Monday    384058268 352   
7  7       Monday    370545015 339   
8  8       Monday    365976832 363   
9  9       Monday    418441855 270   
10 10      Monday    406004208 359   
11 11      Monday    423280315 348   
12 12      Monday    340901327 339   
13 1       Tuesday   400817830 350   
14 2       Tuesday   418837483 322   
15 3       Tuesday   437823712 361   
16 4       Tuesday   426282820 351   
17 5       Tuesday   427458624 356   
18 6       Tuesday   419170028 352   
19 7       Tuesday   435046039 351   
20 8       Tuesday   382871293 363   
21 9       Tuesday   449313964 351   
22 10      Tuesday   444263096 365   
23 11      Tuesday   505136914 308   
24 12      Tuesday   353954234 352   
25 1       Wednesday 423543663 352   
26 2       Wednesday 417507312 320   
27 3       Wednesday 443331437 362   
28 4       Wednesday 450415557 351   
29 5       Wednesday 442514775 358   
30 6       Wednesday 417317212 348   
31 7       Wednesday 450809745 349   
32 8       Wednesday 415405769 357   
33 9       Wednesday 452873849 348   
34 10      Wednesday 481962775 360   
35 11      Wednesday 427695613 351   
36 12      Wednesday 372965531 349   
37 1       Thursday  433573990 351   
38 2       Thursday  421705295 321   
39 3       Thursday  434376372 361   
40 4       Thursday  454656450 352   
41 5       Thursday  450360702 356   
42 6       Thursday  409526069 351   
43 7       Thursday  464447007 351   
44 8       Thursday  408205609 362   
45 9       Thursday  432302687 350   
46 10      Thursday  485194151 364   
47 11      Thursday  482425371 270   
48 12      Thursday  356492202 350   
49 1       Friday    420148129 352   
50 2       Friday    435102880 320   
51 3       Friday    423954229 338   
52 4       Friday    432538397 293   
53 5       Friday    419002211 357   
54 6       Friday    428063715 350   
55 7       Friday    394211784 342   
56 8       Friday    380202343 363   
57 9       Friday    415383236 349   
58 10      Friday    478938352 364   
59 11      Friday    376480878 352   
60 12      Friday    351334635 341
```


```{r eval=FALSE}
> ## Compute the the average closing price for each day of the week for each month, using volume as frequency weights
> rxCube(Close ~ F(Month):DayOfWeek, data = djiXdf, fweights = "Volume")
Rows Read: 6000, Total Rows Processed: 6000, Total Chunk Time: 0.002 seconds
Rows Read: 6000, Total Rows Processed: 12000, Total Chunk Time: 0.002 seconds
Rows Read: 6000, Total Rows Processed: 18000, Total Chunk Time: 0.002 seconds
Rows Read: 2636, Total Rows Processed: 20636, Total Chunk Time: 0.001 seconds 
Computation time: 0.009 seconds.
Call:
rxCube(formula = Close ~ F(Month):DayOfWeek, data = djiXdf, fweights = "Volume")

Cube Results for: Close ~ F(Month):DayOfWeek
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/DJIAdaily.xdf
Frequency weights: Volume
Dependent variable(s): Close
Sum of weights of valid observations: 8612070155344
Number of missing observations: 0 
Statistic: Close means 
 
   F_Month DayOfWeek Close    Counts      
1  1       Monday    9447.143 100453160640
2  2       Monday    9318.145  91926419616
3  3       Monday    9290.767 155211019552
4  4       Monday    9722.881 135360420160
5  5       Monday    9906.000 114411520096
6  6       Monday    9752.588 135188510400
7  7       Monday    9798.827 125614759936
8  8       Monday    9843.677 132849590176
9  9       Monday    9889.898 112979300912
10 10      Monday    9729.465 145755510672
11 11      Monday    9819.758 147301549648
12 12      Monday    9473.085 115565549936
13 1       Tuesday   9609.819 140286240336
14 2       Tuesday   9385.130 134865669392
15 3       Tuesday   9281.642 158054359936
16 4       Tuesday   9678.512 149625269760
17 5       Tuesday   9887.023 152175269984
18 6       Tuesday   9721.678 147547849776
19 7       Tuesday   9854.674 152701159648
20 8       Tuesday   9790.362 138982279296
21 9       Tuesday   9893.286 157709201312
22 10      Tuesday   9653.250 162156030160
23 11      Tuesday   9832.750 155582169504
24 12      Tuesday   9486.524 124591890400
25 1       Wednesday 9713.556 149087369328
26 2       Wednesday 9328.339 133602339728
27 3       Wednesday 9352.612 160485980304
28 4       Wednesday 9668.298 158095860384
29 5       Wednesday 9847.209 158420289616
30 6       Wednesday 9770.470 145226389792
31 7       Wednesday 9645.457 157332601008
32 8       Wednesday 9987.167 148299859456
33 9       Wednesday 9826.807 157600099424
34 10      Wednesday 9666.057 173506599040
35 11      Wednesday 9757.541 150121160240
36 12      Wednesday 9671.284 130164970256
37 1       Thursday  9666.235 152184470496
38 2       Thursday  9265.331 135367399680
39 3       Thursday  9407.788 156809870400
40 4       Thursday  9625.000 160039070288
41 5       Thursday  9908.007 160328410080
42 6       Thursday  9815.480 143743650048
43 7       Thursday  9798.228 163020899488
44 8       Thursday  9968.487 147770430432
45 9       Thursday  9836.212 151305940528
46 10      Thursday  9518.674 176610670944
47 11      Thursday  9858.042 130254850080
48 12      Thursday  9701.425 124772270800
49 1       Friday    9520.421 147892141520
50 2       Friday    9356.839 139232921600
51 3       Friday    9292.174 143296529568
52 4       Friday    9820.420 126733750400
53 5       Friday    9721.381 149583789488
54 6       Friday    9878.071 149822300080
55 7       Friday    9759.297 134820430224
56 8       Friday    9942.661 138013450560
57 9       Friday    9875.476 144968749200
58 10      Friday    9523.039 174333560160
59 11      Friday    9770.669 132521269088
60 12      Friday    9642.693 119805110368
> 
```


***

# CHAPTER 3
## Data Manipulation 

```{r eval=FALSE}
sampleDataDir<-rxOptions()[['sampleDataDir']]
list.files(path=sampleDataDir, pattern = "mortDefaultSmall2.*.csv")

mort2001<-file.path ...
rxImport(inData=mort2001, outfile=mort2001Xdf, overwrite = TRUE)

rxGetInfo(mort2001Xdf, getVarInfo = TRUE)
rxHistogram(~ccDebt, data=mortData, xNumTicks=100)

rxDataFrameToXdf(data=myData, outfile...)

rxDataStep(inData=mortData, outFile=mortData,
    transforms=list(experiencedWorked=yearsEmploy>=5),
    append="cols", overwrite=TRUE)
rxGetVarInfo(mortData)

> ## Get information on mortData
> rxGetInfo(mortData)
File name: /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/mortDefaultSmall.xdf 
Number of observations: 1e+05 
Number of variables: 6 
Number of blocks: 10 
Compression type: zlib 
> ## Set up my personal copy of the data:
> myMortData <- "myMD.xdf"
> ## Create the transform
> rxDataStep(inData = mortData, outFile = myMortData, transforms = list(highDebtRow = ccDebt > 8000))

The 'outFile' already exists.  Specify 'overwrite = TRUE' to overwrite.
Error : 
> ## Get the variable information
> rxGetVarInfo(myMortData)
Var 1: creditScore, Type: integer, Low/High: (470, 925)
Var 2: houseAge, Type: integer, Low/High: (0, 40)
Var 3: yearsEmploy, Type: integer, Low/High: (0, 14)
Var 4: ccDebt, Type: integer, Low/High: (0, 14094)
Var 5: year, Type: integer, Low/High: (2000, 2009)
Var 6: default, Type: integer, Low/High: (0, 1)
Var 7: highDebtRow, Type: logical, Low/High: (0, 1)
Var 8: newHouse, Type: logical, Low/High: (0, 1)
Var 9: ccsXhd, Type: integer, Low/High: (0, 887)
> ## Get the proportion of values that are 1.
> rxSummary(~highDebtRow, data = myMortData)
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: Less than .001 seconds 
Computation time: 0.007 seconds.
Call:
rxSummary(formula = ~highDebtRow, data = myMortData)

Summary Statistics Results for: ~highDebtRow
Data: myMortData (RxXdfData Data Source)
File name: myMD.xdf
Number of valid observations: 1e+05 
 
 Name        Mean    StdDev    Min Max ValidObs MissingObs
 highDebtRow 0.06705 0.2501098 0   1   1e+05    0         
> ## Compute multiple transforms!
> rxDataStep(inData = myMortData, outFile = myMortData, transforms = list(newHouse = houseAge < 10, ccsXhd = creditScore * highDebtRow), append = "cols", overwrite = TRUE)
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.007 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.007 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 
> 


# Revolution Big Data - Data Manipulation Session 2

rxGetVarInfo(mortData)$creditScore
names(rxGetVarInfo(mortData)$creditScore)
minCS<-rxGetVarInfo(mortData)$creditScore$low
maxCS<-rxGetVarInfo(mortData)$creditScore$high

simplyScale <- function(dataList) {
  dataList[['sclCreditScore']] <- (dataList[['creditScore']] - minCreditScore)/(maxCreditScore-minCreditScore)
  dataList[['moreThanHalf']] <- (dataList[['sclCreditScore']] > 0.5)
  dataList[['newConst']] <- rep(23, length.out=length(dataList[[1]]))
  return(dataList)
}

rxDataStep(inData = myMortData, outFile = myMortData, transformFunc = simplyScale, 
    transformObjects = list(minCreditScore = minCS, maxCreditScore = maxCS), 
    append = "cols", overwrite = TRUE)
rxGetVarInfo(data = mortData, getVarInfo = TRUE)

rxSummary(~sclCreditScore, data=mortData)

rxHistogram(~sclCreditScore, data=mortData, xNumTicks = 15)


> ## Compute the summary statistics
> (csSummary <- rxSummary(~creditScore, data = mortData))
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: Less than .001 seconds 
Computation time: 0.009 seconds.
Call:
rxSummary(formula = ~creditScore, data = mortData)

Summary Statistics Results for: ~creditScore
Data: mortData (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/mortDefaultSmall.xdf
Number of valid observations: 1e+05 
 
 Name        Mean     StdDev   Min Max ValidObs MissingObs
 creditScore 699.8854 50.15867 470 925 1e+05    0         
> ## Extract the mean and std. deviation
> meanCS <- csSummary$sDataFrame$Mean[1]
> sdCS <- csSummary$sDataFrame$StdDev[1]
> ## Create a function to compute the scaled variable
> scaleCS <- function(mylist) {
    mylist[["scaledCreditScore"]] <- (mylist[["creditScore"]] - myCenter)/myScale
    return(mylist)
}
> ## Run it with rxDataStep
> myMortData <- "myMD.xdf"
> rxDataStep(inData = mortData, outFile = myMortData, transformFunc = scaleCS, transformObjects = list(myCenter = meanCS, myScale = sdCS))
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.007 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 
> ## Check the new variable:
> rxGetVarInfo(myMortData)
Var 1: creditScore, Type: integer, Low/High: (470, 925)
Var 2: houseAge, Type: integer, Low/High: (0, 40)
Var 3: yearsEmploy, Type: integer, Low/High: (0, 14)
Var 4: ccDebt, Type: integer, Low/High: (0, 14094)
Var 5: year, Type: integer, Low/High: (2000, 2009)
Var 6: default, Type: integer, Low/High: (0, 1)
Var 7: scaledCreditScore, Type: numeric, Low/High: (-4.5832, 4.4880)
> rxSummary(~scaledCreditScore, data = myMortData)
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: Less than .001 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: Less than .001 seconds 
Computation time: 0.010 seconds.
Call:
rxSummary(formula = ~scaledCreditScore, data = myMortData)

Summary Statistics Results for: ~scaledCreditScore
Data: myMortData (RxXdfData Data Source)
File name: myMD.xdf
Number of valid observations: 1e+05 
 
 Name              Mean         StdDev Min       Max      ValidObs MissingObs
 scaledCreditScore 8.741097e-15 1      -4.583165 4.488049 1e+05    0         
> 
```

***

# CHAPTER 4
## Data Analysis
Big data analytics with RRE: Modeling airline data

```{r eval=FALSE}

#rxLinMod similar to R's lm()

sampleDataDir <- rxGetOption('sampleDataDir')
list.files(path = sampleDataDir, pattern="*.csv")

# identify input file
inputFile <- file.path(sampleDataDir, "AirlineDemoSmall.csv")
# set output file
airDS <- "xdf/ADS.xdf"

# create xdf object file
rxImport(inData = inputFile , outFile = airDS, colInfo=.... 

# extract information
rxInfo(airDS, getVarsInfo = TRUE)

# build up summary
rxSummary(~., data= airDS)

# display the delay per day of week
rxSummary(ArrDelay~DayOfWeek, data=airDS)

# or via rxCube
delayCube <- rxCube <- ArrDelay ~ DayOfWeek, data=airDS, means=TRUE)

# examine the distribution of Departure Time
rxHistogram(~CRSDepTime, data=airDS, numBreaks = 25, xNumTicks = 13)

# examine the distribution of Arrival Delay (heavily skewed)
rxHistogram(~ArrDelay, data=airDS, xNumTicks = 16)




> # Declare the file paths for the csv and xdf files
> myAirlineCsv <- file.path(rxGetOption("sampleDataDir"), "AirlineDemoSmall.csv")
> myAirlineXdf <- "ADS.xdf"
> # Use rxImport to import the data into xdf format
> rxImport(inData = myAirlineCsv, outFile = myAirlineXdf, overwrite = TRUE, colInfo = list(DayOfWeek = list(type = "factor", levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))))
Rows Read: 500000, Total Rows Processed: 500000, Total Chunk Time: 2.830 seconds
Rows Read: 100000, Total Rows Processed: 600000, Total Chunk Time: 0.696 seconds 




> # Summarize arrival delay for each day of the week.
> rxSummary(ArrDelay ~ DayOfWeek, data = myAirlineXdf)
Rows Read: 200000, Total Rows Processed: 200000, Total Chunk Time: 0.006 seconds
Rows Read: 200000, Total Rows Processed: 400000, Total Chunk Time: 0.009 seconds
Rows Read: 200000, Total Rows Processed: 600000, Total Chunk Time: 0.011 seconds 
Computation time: 0.035 seconds.
Call:
rxSummary(formula = ArrDelay ~ DayOfWeek, data = myAirlineXdf)

Summary Statistics Results for: ArrDelay ~ DayOfWeek
Data: myAirlineXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/AirlineDemoSmall.xdf
Dependent variable(s): ArrDelay
Number of valid observations: 6e+05 
 
 Name               Mean     StdDev   Min Max  ValidObs MissingObs
 ArrDelay:DayOfWeek 11.31794 40.68854 -86 1490 582628   17372     

Statistics by category (7 categories):

 Category                         DayOfWeek Means     StdDev   Min Max 
 ArrDelay for DayOfWeek=Monday    Monday    12.025604 40.02463 -76 1017
 ArrDelay for DayOfWeek=Tuesday   Tuesday   11.293808 43.66269 -70 1143
 ArrDelay for DayOfWeek=Wednesday Wednesday 10.156539 39.58803 -81 1166
 ArrDelay for DayOfWeek=Thursday  Thursday   8.658007 36.74724 -58 1053
 ArrDelay for DayOfWeek=Friday    Friday    14.804335 41.79260 -78 1490
 ArrDelay for DayOfWeek=Saturday  Saturday  11.875326 45.24540 -73 1370
 ArrDelay for DayOfWeek=Sunday    Sunday    10.331806 37.33348 -86 1202
 ValidObs
 95298   
 74011   
 76786   
 79145   
 80142   
 83851   
 93395   
> ## Vizualize the arrival delay histogram
> #rxHistogram(~ArrDelay | DayOfWeek, data = myAirlineXdf)
> rxHistogram(~ArrDelay, data = myAirlineXdf)
Rows Read: 200000, Total Rows Processed: 200000, Total Chunk Time: 0.071 seconds
Rows Read: 200000, Total Rows Processed: 400000, Total Chunk Time: 0.074 seconds
Rows Read: 200000, Total Rows Processed: 600000, Total Chunk Time: 0.073 seconds 
Computation time: 0.227 seconds.
> 


# building up the linear model
# Predict Arrival Delay by Day of Week
arrDelayLm1 <- rxLinMod (ArrDelay ~ DayOfWeek, data=airDS)
summary(arrDelayLm1)
# delays compared to the reference group (dropped level, Sunday)


arrDelayLm1df <- rxLinMod (ArrDelay ~ DayOfWeek, data=airDS, dropFirst = TRUE)
# different coefficients (dropped level, Monday)

# run analysis using the partition inverse
arrDelayLm1cube <- rxLinMod (ArrDelay ~ DayOfWeek, data=airDS, cube = TRUE)

# cube = TRUE uses no Intercept!
summary(arrDelayLm1cube) 

# use several field names
# Use the interaction:
arrDelayLm4 <- rxLinMod (ArrDelay ~ DayOfWeek:CRSDepTime, data=airDS)

# Main Effects and Interactions
arrDelayLm4b <- rxLinMod (ArrDelay ~ DayOfWeek*CRSDepTime, data=airDS)

# predicting the log of absolute departure delay by the interaction of the day of the week and CRSDepTime as factor
arrDelayLm1cube <- rxLinMod (log(abs(ArrDelay)) ~ DayOfWeek:F(CRSDepTime), data=airDS, cube = TRUE)


> ## predict arrival delay by day of the week:
> myLM1 <- rxLinMod(ArrDelay ~ DayOfWeek, data = myAirlineXdf)
Rows Read: 200000, Total Rows Processed: 200000, Total Chunk Time: 0.008 seconds
Rows Read: 200000, Total Rows Processed: 400000, Total Chunk Time: 0.010 seconds
Rows Read: 200000, Total Rows Processed: 600000, Total Chunk Time: 0.011 seconds 
Computation time: 0.036 seconds.
> ## summarize the model
> summary(myLM1)
Call:
rxLinMod(formula = ArrDelay ~ DayOfWeek, data = myAirlineXdf)

Linear Regression Results for: ArrDelay ~ DayOfWeek
Data: myAirlineXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/AirlineDemoSmall.xdf
Dependent variable(s): ArrDelay
Total independent variables: 8 (Including number dropped: 1)
Number of valid observations: 582628
Number of missing observations: 17372 
 
Coefficients: (1 not defined because of singularities)
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)          10.3318     0.1330  77.673 2.22e-16 ***
DayOfWeek=Monday      1.6938     0.1872   9.049 2.22e-16 ***
DayOfWeek=Tuesday     0.9620     0.2001   4.809 1.52e-06 ***
DayOfWeek=Wednesday  -0.1753     0.1980  -0.885    0.376    
DayOfWeek=Thursday   -1.6738     0.1964  -8.522 2.22e-16 ***
DayOfWeek=Friday      4.4725     0.1957  22.850 2.22e-16 ***
DayOfWeek=Saturday    1.5435     0.1934   7.981 2.22e-16 ***
DayOfWeek=Sunday     Dropped    Dropped Dropped  Dropped    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 40.65 on 582621 degrees of freedom
Multiple R-squared: 0.001869 
Adjusted R-squared: 0.001858 
F-statistic: 181.8 on 6 and 582621 DF,  p-value: < 2.2e-16 
Condition number: 10.5595 
> ## Use the transforms argument to create a factor variable associated with departure time "on the fly,"
> ## predict Arrival Delay by the interaction between Day of the week and that new factor variable.
> myLM2 <- rxLinMod(ArrDelay ~ DayOfWeek, data = myAirlineXdf, transforms = list(catDepTime = cut(CRSDepTime, breaks = seq(from = 5, to = 23, by = 2))), cube = TRUE)
Rows Read: 200000, Total Rows Processed: 200000, Total Chunk Time: 0.082 seconds
Rows Read: 200000, Total Rows Processed: 400000, Total Chunk Time: 0.080 seconds
Rows Read: 200000, Total Rows Processed: 600000, Total Chunk Time: 0.076 seconds 
Computation time: 0.255 seconds.
> ## summarize the model
> summary(myLM2)
Call:
rxLinMod(formula = ArrDelay ~ DayOfWeek, data = myAirlineXdf, 
    cube = TRUE, transforms = list(catDepTime = cut(CRSDepTime, 
        breaks = seq(from = 5, to = 23, by = 2))))

Cube Linear Regression Results for: ArrDelay ~ DayOfWeek
Data: myAirlineXdf (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/AirlineDemoSmall.xdf
Dependent variable(s): ArrDelay
Total independent variables: 7 
Number of valid observations: 582628
Number of missing observations: 17372 
 
Coefficients:
                    Estimate Std. Error t value Pr(>|t|)     | Counts
DayOfWeek=Monday     12.0256     0.1317   91.32 2.22e-16 *** |  95298
DayOfWeek=Tuesday    11.2938     0.1494   75.58 2.22e-16 *** |  74011
DayOfWeek=Wednesday  10.1565     0.1467   69.23 2.22e-16 *** |  76786
DayOfWeek=Thursday    8.6580     0.1445   59.92 2.22e-16 *** |  79145
DayOfWeek=Friday     14.8043     0.1436  103.10 2.22e-16 *** |  80142
DayOfWeek=Saturday   11.8753     0.1404   84.59 2.22e-16 *** |  83851
DayOfWeek=Sunday     10.3318     0.1330   77.67 2.22e-16 *** |  93395
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 40.65 on 582621 degrees of freedom
Multiple R-squared: 0.001869 (as if intercept included)
Adjusted R-squared: 0.001858 
F-statistic: 181.8 on 6 and 582621 DF,  p-value: < 2.2e-16 
Condition number: 1 
> 
```


***

##Prediction and Crossvalidations

```{r eval=FALSE}
> args(RevoScaleR::rxPredict.default)
function (modelObject, data=NULL, outData = NULL....

# use the linear regression model that combines the day of the week
# with the 2 hours bin for the hours where air-traffic is at its peaks.
> arDelayLm <- rxLinMod(ArrDelay ~ DayOfWeek:depTimeCat,
		data = airDS, cube = TRUE,
		transforms=list( depTimeCat = cut(CRSDepTime, breaks=seq(from=6, to=23, by=2))
		)
)

# large output
summary(arDelayLm)
#63 coefficients

# call prediction function, somehow similar to R's predict
rxPredict(modelObject = arrDelayLm, 
			data = airDS, 
			outData = airDS, 
			overwrite = TRUE)

# the linear regression model generated a new variable called ArrDelay_Pred 
rxGetVarInfo(airDS)
# Var 1...
# ArrDelay_Pred 


# compute resiguals
# call prediction function, somehow similar to R's predict
rxPredict(modelObject = arrDelayLm, 
			data = airDS, 
			outData = airDS, 
			computeResiduals = TRUE,
			overwrite = TRUE)
# the residuals are also there
rxGetVarInfo(airDS)
# Var 1...
# ArrDelay_Pred 
# ArrDelay_Resid 

# extremely skewed residuals
rxHistogram( ~ ArrDelay_Resid, data = airDS)
# we might need to adjust the data points for model

# split data into train/test
rxDataStep(inData = airDS, outFile = airDS, transforms = list(urv = runif(length(DayOfWeek))), append = "cols", overwrite=TRUE)

rxHistogram( ~urv, data = airDS, xNumTicks = 10)

# then use cut to split the data
rxDataStep(inData = airDS, outFile = airDS, 
			transforms = list(TrainTest = cut(
					urv, 
					breaks = c(0, 0.8, 1), 
					labels = c("Train", "Test")
			)), 
			append = "cols", overwrite=TRUE)

# 80% 20% split
rxSummary(~TrainTest, data = airDS)

# now split it into several xds files
mySplit <- rxSplit(inData = airDS, splitByFactor = "TrainTest", 
			rowSelection = ArrDelay < 450,
			overwrite = TRUE)

# two files were created
names(mySplit)

# Computing Std. Errors
rxPredict(modelObject = arrDelayLm.train, 
			data = testDS, 
			outData = testDS, 
			computeResiduals = TRUE,
			computeStdErrors = TRUE,
			overwrite = TRUE)
# but we must specify the covCoef = TRUE argument with the linear Model
> arDelayLm <- rxLinMod(ArrDelay ~ DayOfWeek:depTimeCat,
		data = airDS, cube = TRUE,
		transforms=list( depTimeCat = cut(CRSDepTime, breaks=seq(from=6, to=23, by=2)),
		covCoef = TRUE
		)
)
```

***

## Generating Predictions and Residuals
## Extract model predictions using rxPredict().

```{r eval=FALSE}
summary(myLM2)

mySubset <- rxDataStep(inData = myAirlineXdf, startRow = 0, numRows = 1000) 

rxPredict(modelObject = myLM2, 
        data = myAirlineXdf, 
        outData = mySubset, 
        writeModelVars = TRUE,
        computeResiduals = FALSE, 
        overwrite = TRUE)

#rxGetVarInfo(myAirlineXdf)

#rxPredict(modelObject = myLM2, 
#        data = myAirlineXdf, outData = myAirlineXdf, 
#        computeResiduals = TRUE, 
#        overwrite = TRUE)
        
#rxGetVarInfo(myAirlineXdf)


## summarize model first
summary(myLM2)

## path to new dataset storing predictions
myNewADS <- "myNEWADS.xdf"

## generate predictions
rxPredict(modelObject = myLM2, data = myAirlineXdf, 
          outData = myNewADS, 
          writeModelVars = TRUE)
## get information on the new dataset
rxGetInfo(myNewADS, getVarInfo = TRUE)

## Generate residuals.
rxPredict(modelObject = myLM2, data = myAirlineXdf, 
          outData = myNewADS, 
          writeModelVars = TRUE, 
          computeResiduals = TRUE, 
          overwrite = TRUE)
## get information on the new dataset
rxGetInfo(myNewADS, getVarInfo = TRUE)
```

***


#Logistic Regression and the GLM


```{r eval=FALSE}
# set the path
mortXdf <- file.path(rxGetOption('sampleDataDir'), 'mortDefaultSmall.xdf')

# review the data
rxGetInfo(data = mortXdf, getVarInfo = TRUE)
# 100k vars on 6 vars


args(rxLogit)
## rxLogit(formula, data, pweights = NULL, fweights = NULL, cube = FALSE,
##        cubePredictions = FALSE, variableSelection = list(), rowSelection = NULL, 
##        transforms = NULL, transformObjects = NULL,
##        transformFunc = NULL, transformVars = NULL, 
##        transformPackages = NULL, transformEnvir = NULL,   
##        dropFirst = FALSE, dropMain = rxGetOption("dropMain"),
##        covCoef = FALSE, covData = FALSE, covariance = FALSE,
##        initialValues = NULL, 
##        coefLabelStyle = rxGetOption("coefLabelStyle"),
##        blocksPerRead = rxGetOption("blocksPerRead"), 
##        maxIterations = 25, coeffTolerance = 1e-06, 
##        gradientTolerance = 1e-06, objectiveFunctionTolerance = 1e-08,
##        reportProgress = rxGetOption("reportProgress"), verbose = 0,
##        computeContext = rxGetOption("computeContext"),
##        ...)

# example of using rxLogit
logitModel1 <- rxLogit(default ~F(year) + ccDebt + creditScore + houseAge + yearsEmploy, 
		data= mortXdf,
		dropFirst = TRUE)

# extract results
summary(logitModel1)

dataWithPredictions <- rxPredict(modelObject = logitModel1, 
		data = newData, 
		outData = newData, 
		type="response")

dataWithPredictions

# more general rxGlm glm

# 5% sample of the US Census
bigCensusData <-file.path(dataPath, 'CensusPCT2000.xdf')
rxGetInfo(bigCensusData)
# number of observations: 14 million

propinFile <- file.path('xdf/CensusPropertyIns.xdf')
rxDataStep(inData = bigCensusData,
		rowSelection = (related == "Head/Household") & (age>20) & (age<90),
		varsToKeep = c("propinsr", "age", "sex", "region", "perwt"),
		outFile = propinFile,
		blocksPerRead = 10,
		overwrite = TRUE)

rxGetInfoXdf(propinFile, gerVarInfo = TRUE)

# large level of levels
rxGetInfoXdf(~region, data = propinFile)

# re-level
regionLevels <- list (....)
rxFactors(inData= propinFile, outFile = propinFile, factorInfo = list(region=list(newLevels = regionLevels)

# fewer level of levels
rxGetInfoXdf(~region, data = propinFile)

# predict property insurance cost
rxHistogram(~propinsr, data=propinFile, pweights="perwt", numBreaks = 50)

# predict property insurance cost
rxHistogram(~propinsr, data=propinFile, pweights="perwt", numBreaks = 50)

# use a tweedie distribution
propinGlm <- rGlm(propinsr~sex+F(age) + region,
		pweights = "perwt",
		data=propinFile,
		family = rxTweedie(var.power=1.5),
		dropFirst = TRUE
		)

#look at coefficients
printCoefmat(coef(summary(propinGlm)))

# all age values are significants

# plot predictions
# simulate data
predData <- rxPredict(propinGlm,
		data = predData,
		outData = predData,
		predVarNames=c('predicted'),
		overwrite=TRUE)	

# type of predictions for linear models
# defaults to "response" - places the predicted variable on the scale of the response variable
# different than R's glm predict, which defaults on the link prediction on the scale of the linear predictor

rxLinePlot(predicted~age|region, group=sex, data=predData)
```

***

# Exercises

```{r eval=FALSE}
> # look at the meta data
> rxGetInfo(mortData, getVarInfo = TRUE)
File name: /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/mortDefaultSmall.xdf 
Number of observations: 1e+05 
Number of variables: 6 
Number of blocks: 10 
Compression type: zlib 
Variable information: 
Var 1: creditScore, Type: integer, Low/High: (470, 925)
Var 2: houseAge, Type: integer, Low/High: (0, 40)
Var 3: yearsEmploy, Type: integer, Low/High: (0, 14)
Var 4: ccDebt, Type: integer, Low/High: (0, 14094)
Var 5: year, Type: integer, Low/High: (2000, 2009)
Var 6: default, Type: integer, Low/High: (0, 1)
> # Construct the logit model
> logitModel <- rxLogit(formula = default ~ houseAge + F(year) + ccDebt + creditScore + yearsEmploy, data = mortData)
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.005 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.005 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.005 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.005 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.005 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.005 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.005 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.005 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.005 seconds 

Starting values (iteration 1) time: 0.059 secs.
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 

Iteration 2 time: 0.086 secs.
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 

Iteration 3 time: 0.083 secs.
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 

Iteration 4 time: 0.085 secs.
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 

Iteration 5 time: 0.082 secs.
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 

Iteration 6 time: 0.083 secs.
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 

Iteration 7 time: 0.084 secs.
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 

Iteration 8 time: 0.079 secs.
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.007 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.007 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 

Iteration 9 time: 0.078 secs.
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.004 seconds
Rows Read: 10000, Total Rows Processed: 20000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 30000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 40000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 50000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 60000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 70000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 80000, Total Chunk Time: 0.008 seconds
Rows Read: 10000, Total Rows Processed: 90000, Total Chunk Time: 0.009 seconds
Rows Read: 10000, Total Rows Processed: 100000, Total Chunk Time: 0.008 seconds 

Iteration 10 time: 0.082 secs.

Elapsed computation time: 0.803 secs.
> # Summarize the result contained in logitModel1
> summary(logitModel)
Call:
rxLogit(formula = default ~ houseAge + F(year) + ccDebt + creditScore + 
    yearsEmploy, data = mortData)

Logistic Regression Results for: default ~ houseAge + F(year) + ccDebt
    + creditScore + yearsEmploy
Data: mortData (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/mortDefaultSmall.xdf
Dependent variable(s): default
Total independent variables: 15 (Including number dropped: 1)
Number of valid observations: 1e+05
Number of missing observations: 0 
-2*LogLikelihood: 2946.1416 (Residual deviance on 99986 degrees of freedom)
 
Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -6.939e+00  7.957e-01  -8.720 2.22e-16 ***
houseAge     2.877e-02  7.036e-03   4.089 4.33e-05 ***
F_year=2000 -3.738e+00  3.275e-01 -11.413 2.22e-16 ***
F_year=2001 -2.701e+00  2.268e-01 -11.906 2.22e-16 ***
F_year=2002 -3.798e+00  3.386e-01 -11.216 2.22e-16 ***
F_year=2003 -4.644e+00  5.150e-01  -9.018 2.22e-16 ***
F_year=2004 -4.492e+00  4.400e-01 -10.208 2.22e-16 ***
F_year=2005 -4.533e+00  4.665e-01  -9.717 2.22e-16 ***
F_year=2006 -4.544e+00  4.787e-01  -9.491 2.22e-16 ***
F_year=2007 -3.130e+00  2.629e-01 -11.905 2.22e-16 ***
F_year=2008 -5.695e-01  1.230e-01  -4.631 3.64e-06 ***
F_year=2009    Dropped    Dropped Dropped  Dropped    
ccDebt       1.320e-03  3.932e-05  33.574 2.22e-16 ***
creditScore -7.824e-03  1.081e-03  -7.240 2.22e-16 ***
yearsEmploy -2.675e-01  2.741e-02  -9.762 2.22e-16 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

Condition number of final variance-covariance matrix: 6.2216 
Number of iterations: 10
> 




> ## Summarize the model
> summary(logitModel)
Call:
rxLogit(formula = default ~ houseAge + F(year) + ccDebt + creditScore + 
    yearsEmploy, data = mortData, dropFirst = T)

Logistic Regression Results for: default ~ houseAge + F(year) + ccDebt
    + creditScore + yearsEmploy
Data: mortData (RxXdfData Data Source)
File name:
    /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/mortDefaultSmall.xdf
Dependent variable(s): default
Total independent variables: 15 (Including number dropped: 1)
Number of valid observations: 1e+05
Number of missing observations: 0 
-2*LogLikelihood: 2946.1416 (Residual deviance on 99986 degrees of freedom)
 
Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.068e+01  8.682e-01 -12.298 2.22e-16 ***
houseAge     2.877e-02  7.036e-03   4.089 4.33e-05 ***
F_year=2000    Dropped    Dropped Dropped  Dropped    
F_year=2001  1.037e+00  3.773e-01   2.748    0.006 ** 
F_year=2002 -5.989e-02  4.514e-01  -0.133    0.894    
F_year=2003 -9.066e-01  5.958e-01  -1.522    0.128    
F_year=2004 -7.543e-01  5.304e-01  -1.422    0.155    
F_year=2005 -7.952e-01  5.536e-01  -1.436    0.151    
F_year=2006 -8.059e-01  5.636e-01  -1.430    0.153    
F_year=2007  6.074e-01  3.988e-01   1.523    0.128    
F_year=2008  3.168e+00  3.306e-01   9.584 2.22e-16 ***
F_year=2009  3.738e+00  3.275e-01  11.413 2.22e-16 ***
ccDebt       1.320e-03  3.932e-05  33.574 2.22e-16 ***
creditScore -7.824e-03  1.081e-03  -7.240 2.22e-16 ***
yearsEmploy -2.675e-01  2.741e-02  -9.762 2.22e-16 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

Condition number of final variance-covariance matrix: 110.2895 
Number of iterations: 10
> ## view the first few rows
> head(newData)
  year ccDebt creditScore houseAge yearsEmploy
1 2006   1000         700       10           7
2 2006   1000         700       30           7
3 2006   1000         800       10           7
4 2006   1000         800       30           7
5 2009  10000         700       10           7
6 2009  10000         700       30           7
> # Make predictions
> dataWithPredictions <- rxPredict(modelObject = logitModel, data = newData, outData = newData, type = "response")
Rows Read: 8, Total Rows Processed: 8, Total Chunk Time: 0.002 seconds 
> ## view the predictions
> dataWithPredictions
  year ccDebt creditScore houseAge yearsEmploy default_Pred
1 2006   1000         700       10           7 3.309304e-08
2 2006   1000         700       30           7 5.883328e-08
3 2006   1000         800       10           7 1.513391e-08
4 2006   1000         800       30           7 2.690528e-08
5 2009  10000         700       10           7 3.098985e-01
6 2009  10000         700       30           7 4.439345e-01
7 2009  10000         800       10           7 1.703740e-01
8 2009  10000         800       30           7 2.674509e-01
> 


#Higher Default Probability
#
#According to the model estimated in logitModel, in which year(s) does having a house older than 30 years correspond with a higher probability of default than having a house less than 10 years old? Select all that apply.
> sum<-summary(logitModel)
> round(sum$coefficients,4)

(Intercept) -10.6763     0.8682 -12.2976   0.0000
houseAge      0.0288     0.0070   4.0891   0.0000
F_year=2000       NA         NA       NA       NA
F_year=2001   1.0367     0.3773   2.7478   0.0060
F_year=2002  -0.0599     0.4514  -0.1327   0.8945
F_year=2003  -0.9066     0.5958  -1.5217   0.1281
F_year=2004  -0.7543     0.5304  -1.4220   0.1550
F_year=2005  -0.7952     0.5536  -1.4364   0.1509
F_year=2006  -0.8059     0.5636  -1.4299   0.1527
F_year=2007   0.6074     0.3988   1.5230   0.1278
F_year=2008   3.1681     0.3306   9.5843   0.0000
F_year=2009   3.7376     0.3275  11.4132   0.0000
ccDebt        0.0013     0.0000  33.5745   0.0000
creditScore  -0.0078     0.0011  -7.2404   0.0000
yearsEmploy  -0.2675     0.0274  -9.7620   0.0000

# Correct! Because there are no interactions in our estimated model, the main effect of **houseAge** would indicate that, on average, the older the house, the larger the default probability.
 
> round(dataWithPredictions,4)
  year ccDebt creditScore houseAge yearsEmploy default_Pred
1 2006   1000         700       10           7       0.0000
2 2006   1000         700       30           7       0.0000
3 2006   1000         800       10           7       0.0000
4 2006   1000         800       30           7       0.0000
5 2009  10000         700       10           7       0.3099
6 2009  10000         700       30           7       0.4439
7 2009  10000         800       10           7       0.1704
8 2009  10000         800       30           7       0.2675
```

***

# K-means Clustering + Decision tree estimations

```{r eval=FALSE}
# mortgage default dataset
sampleDataDir <- rxGetOption('sampleDataDir')
ndata <- file.path(sampleDataDir, 'mortDefaultSmall.xdf')
mortDefault2<-file.path('xdf/mortDefault2.xdf')

# transform to create a random variable
rxDataStep(inData = ndata, outFile = mortDefault2,
		transforms = list(RandomSample = sample(10, size=.rxNumRows, replace = TRUE)),
		overwrite = TRUE)
rxGetVarInfo(mortDefault2)
#..

args(rxKmeans)
##rxKmeans(formula, data, 
##         outFile = NULL, outColName = ".rxCluster", 
##         writeModelVars = FALSE, extraVarsToWrite = NULL,
##         overwrite = FALSE, numClusters = NULL, centers = NULL, 
##         algorithm = "Lloyd", numStartRows = 0, maxIterations = 1000, 
##         numStarts = 1, rowSelection = NULL, 
##         transforms = NULL, transformObjects = NULL,
##         transformFunc = NULL, transformVars = NULL, 
##         transformPackages = NULL, transformEnvir = NULL,
##         blocksPerRead = rxGetOption("blocksPerRead"),
##         reportProgress = rxGetOption("reportProgress"), verbose = 0,
##         computeContext = rxGetOption("computeContext"),
##         xdfCompressionLevel = rxGetOption("xdfCompressionLevel"), ...)

nd.km <- rxKmeans(formula=~creditScore + houseAge + yearsEmploy + ccDebt + year,
		data = mortDefault2,
		numClusters = 3,
		outFile = mortDefault2,
		algorithm = 'lloyd',
		overwrite = TRUE)

print(nd.km)
#...

# new variable called .rxCluster between 1:3
rxGetVarInfo(mortDefault2)

# extract and plot the clusters
ndDf <- rxXdfToDataFrame(file = mortDefault2, rowSelection = RandomSample ==5)
plot(ndDf[,1:4], col=ndDf$.rxCluster)

# the splis is lead by ccDebt because the variation inside this variable is high and it dominates the cluster splitting
# to avoid this we would need to "level" all the variables 

> ## Examine the mortData dataset
> rxGetInfo(mortData, getVarInfo = TRUE)
File name: /usr/lib64/Revo-7.3/R-3.1.1/lib64/R/library/RevoScaleR/SampleData/mortDefaultSmall.xdf 
Number of observations: 1e+05 
Number of variables: 6 
Number of blocks: 10 
Compression type: zlib 
Variable information: 
Var 1: creditScore, Type: integer, Low/High: (470, 925)
Var 2: houseAge, Type: integer, Low/High: (0, 40)
Var 3: yearsEmploy, Type: integer, Low/High: (0, 14)
Var 4: ccDebt, Type: integer, Low/High: (0, 14094)
Var 5: year, Type: integer, Low/High: (2000, 2009)
Var 6: default, Type: integer, Low/High: (0, 1)
> ## set up a path to a new xdf file
> myNewMortData = "myMDwithKMeans.xdf"
> ## run k-means:
> KMout <- rxKmeans(formula = ~ccDebt + creditScore + houseAge, data = mortData, outFile = myNewMortData, rowSelection = year == 2000, numClusters = 4, writeModelVars = TRUE)

Cannot use 'rowSelection' when appending columns to an existing data source.
Error : 
> print(KMout)
Call:
rxKmeans(formula = ~ccDebt + creditScore + houseAge, data = mortData, 
    outFile = myNewMortData, writeModelVars = TRUE, numClusters = 4, 
    rowSelection = year == 2000)

Data: mortData
Number of valid observations: 10000
Number of missing observations: 0 
Clustering algorithm:  
 
K-means clustering with 4 clusters of sizes 1844, 1425, 3224, 3507

Cluster means:
    ccDebt creditScore houseAge
1 2085.177    698.6274 19.97560
2 8134.783    700.7565 20.01825
3 6038.094    698.9132 20.04156
4 4230.114    700.3975 19.93584

Within cluster sum of squares by cluster:
         1          2          3          4 
1340550664 1128944862  967982253 1090800080 

Available components:
 [1] "centers"       "size"          "withinss"      "valid.obs"    
 [5] "missing.obs"   "numIterations" "tot.withinss"  "totss"        
 [9] "betweenss"     "cluster"       "params"        "formula"      
[13] "call"         
> ## Examine the variables in the new dataset:
> rxGetInfo(myNewMortData, getVarInfo = TRUE)
File name: /tmp/Rserv/conn16352/myMDwithKMeans.xdf 
Number of observations: 10000 
Number of variables: 5 
Number of blocks: 1 
Compression type: zlib 
Variable information: 
Var 1: .rxCluster, Type: integer, Low/High: (1, 4)
Var 2: ccDebt, Type: integer, Low/High: (0, 12275)
Var 3: creditScore, Type: integer, Low/High: (486, 895)
Var 4: houseAge, Type: integer, Low/High: (0, 40)
Var 5: year, Type: integer, Low/High: (2000, 2000)
> ## Summarize the cluster variable:
> rxSummary(~F(.rxCluster), data = myNewMortData)
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.001 seconds 
Computation time: 0.004 seconds.
Call:
rxSummary(formula = ~F(.rxCluster), data = myNewMortData)

Summary Statistics Results for: ~F(.rxCluster)
Data: myNewMortData (RxXdfData Data Source)
File name: myMDwithKMeans.xdf
Number of valid observations: 10000 
 

Category Counts for F_.rxCluster
Number of categories: 4
Number of valid observations: 10000
Number of missing observations: 0

 F_.rxCluster Counts
 1            1844  
 2            1425  
 3            3224  
 4            3507  
> ## read into memory 10% of the data:
> mydf <- rxXdfToDataFrame(myNewMortData, rowSelection = randSamp == 1, varsToDrop = "year", transforms = list(randSamp = sample(10, size = .rxNumRows, replace = TRUE)))
Rows Read: 10000, Total Rows Processed: 10000, Total Chunk Time: 0.005 seconds 

Rows Processed: 1020
Time to read data file: less than .001 secs.
Time to convert to data frame: less than .001 secs.
> ## visualize the clusters:
> plot(mydf[-1], col = mydf$.rxCluster)
> 
```

***

# Decision Trees

```{r eval=FALSE}
# continue using the mortgage defaulting dataset
mortDefault2<-file.path('xdf/mortDefault2.xdf')
rxGetVarInfo(mortDefault2, getVarInfo = TRUE)

# create 10% Test, 90% Train
trainTestFiles <- rxSplit(inData = mortDefault2, 
			transform = list(TrainTest = factor(ifelse(RandomSample<9,'Train','Test'))),
			splitByFactor = "TrainTest", 
			overwrite = TRUE)

# the RevoScale rxDTree uses both regression and classification trees
# decides based on the response variable (numeric -> regression, categorical -> classification)

args(rxDTree)
##rxDTree(formula, data,
##    outFile = NULL, outColName = ".rxNode", writeModelVars = FALSE, extraVarsToWrite = NULL, overwrite = FALSE,
##    pweights = NULL, fweights = NULL, method = NULL, parms = NULL, cost = NULL, 
##    minSplit = max(20, sqrt(numObs)), minBucket = round(minSplit/3), maxDepth = 10, cp = 0,
##    maxCompete = 0, maxSurrogate = 0, useSurrogate = 2, surrogateStyle = 0, xVal = 2,
##    maxNumBins = NULL, maxUnorderedLevels = 32, removeMissings = FALSE, 
##    useSparseCube = rxGetOption("useSparseCube"), pruneCp = 0,
##    rowSelection = NULL, transforms = NULL, transformObjects = NULL, transformFunc = NULL,
##    transformVars = NULL, transformPackages = NULL, transformEnvir = NULL,
##    blocksPerRead = rxGetOption("blocksPerRead"), reportProgress = rxGetOption("reportProgress"),
##    verbose = 0, computeContext = rxGetOption("computeContext"), 
##    xdfCompressionLevel = rxGetOption("xdfCompressionLevel"),
##    ...)

# simple case
treeR <-rxDTree(
		formula = default ~ year + ccDebt + creditScore + houseAge + yearsEmploy
		data = trainTestFiles[[1]], 
		maxdepth = 5)	
(treeR)

# plotting the tree
plot(rxAddInheritance(treeR), uniform = TRUE, margin=0.1)
text(rxAddInheritance(treeR), digits = 2, cex=0.6)
title(main = "Regression Tree for Mortgage Data")	

# or use the interactive viewer from RevoTreeView
library(RevoTreeView)
plot(createTreeView(treeR))

# use rxPredict to estimate the accuracy of the model
rxPredict(treeR, data=trainTestFiles[[2]], outData=trainTestFiles[[2]], overwrite = TRUE, predVarNames = "Pred_R")

#receiver operating characteristic ROC Curve
rxRocCurve(actualVarName = "default", predVarNames = c("Pred_R"), data=trainTestFiles[[2]])
# good performance (high AUC)

# in order to build classification tree we need factors

# Training Dataset
rxFactors(inData = trainTestFiles[[1]], factorInfo = list(defaultFactor = list(varName = "default")),
		outFile = trainTestFiles[[1]], overwrite=TRUE)

# Testing Dataset
rxFactors(inData = trainTestFiles[[2]], factorInfo = list(defaultFactor = list(varName = "default")),
		outFile = trainTestFiles[[2]], overwrite=TRUE)

# check that it is a factor
rxGetVarInfo(trainTestFiles[[1]], getVarInfo = TRUE)

# plot histogram
rHistogram(~defaultFactor, data=trainTestFiles[[1]])

# control <- list(...)
treeC <-rxDTree(
		formula = defaultFactor ~ year + ccDebt + creditScore + houseAge + yearsEmploy
		data = trainTestFiles[[1]], 
		control = control,
		maxNumBins = 15000)	

# plotting the tree
plot(rxAddInheritance(treeC), uniform = TRUE, margin=0.1)
text(rxAddInheritance(treeC), digits = 2, cex=0.6)
title(main = "Classification Tree for Mortgage Data")	

# or use RevoTreeViewer
library(RevoTreeView)
plot(createTreeView(treeC))

# build confusion matrix
conf.mat <- rxCrossTabs(~defaultFactor:Pred_CF, data = trainTestFiles[[2]])

# hits, misses, false alarms and correct rejections
prop.table(conf.mat)

```

# This concludes the Course [Big Data Analysis with Revolution R Enterprise](https://www.datacamp.com/courses/big-data-revolution-r-enterprise-tutorial)
# Continue unlocking the power of the R language for advanced analytics on big data
# [http://www.revolutionanalytics.com/]

